{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-20T17:34:25.748001Z","iopub.execute_input":"2022-07-20T17:34:25.748737Z","iopub.status.idle":"2022-07-20T17:34:25.789388Z","shell.execute_reply.started":"2022-07-20T17:34:25.748635Z","shell.execute_reply":"2022-07-20T17:34:25.787894Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/pyspark-test-data/test1.csv\n/kaggle/input/pyspark-test-data/test2.csv\n/kaggle/input/pyspark-test-data/tips.csv\n/kaggle/input/pyspark-test-data/test3.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pyspark\nimport pyspark","metadata":{"execution":{"iopub.status.busy":"2022-07-20T17:34:25.791098Z","iopub.execute_input":"2022-07-20T17:34:25.791756Z","iopub.status.idle":"2022-07-20T17:35:15.750910Z","shell.execute_reply.started":"2022-07-20T17:34:25.791726Z","shell.execute_reply":"2022-07-20T17:35:15.749568Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.3/281.3 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: py4j==0.10.9.5 in /opt/conda/lib/python3.7/site-packages (from pyspark) (0.10.9.5)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=8ba73e09e2132efa93c0c7129fc928608d4e82bcc69eb933f839a100ef388469\n  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.3.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"* Pyspark Handling Missing Values\n* Dropping Columns\n* Dropping Rows\n* Various Parameter In Dropping functionalities\n* Handling Missing values by Mean, MEdian And Mode","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nspark=SparkSession.builder.appName('Practise').getOrCreate()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T17:35:15.752597Z","iopub.execute_input":"2022-07-20T17:35:15.753942Z","iopub.status.idle":"2022-07-20T17:35:22.072523Z","shell.execute_reply.started":"2022-07-20T17:35:15.753894Z","shell.execute_reply":"2022-07-20T17:35:22.071342Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","output_type":"stream"},{"name":"stdout","text":"22/07/20 17:35:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}]},{"cell_type":"code","source":"df_pyspark=spark.read.csv('../input/pyspark-test-data/test2.csv',header=True,inferSchema=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-20T17:35:22.079458Z","iopub.execute_input":"2022-07-20T17:35:22.080287Z","iopub.status.idle":"2022-07-20T17:35:29.490711Z","shell.execute_reply.started":"2022-07-20T17:35:22.080237Z","shell.execute_reply":"2022-07-20T17:35:29.489620Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df_pyspark.printSchema()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T17:35:29.495696Z","iopub.execute_input":"2022-07-20T17:35:29.496066Z","iopub.status.idle":"2022-07-20T17:35:29.516261Z","shell.execute_reply.started":"2022-07-20T17:35:29.496027Z","shell.execute_reply":"2022-07-20T17:35:29.515333Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"root\n |-- Name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- Experience: integer (nullable = true)\n |-- Salary: integer (nullable = true)\n\n","output_type":"stream"}]},{"cell_type":"code","source":"df_pyspark.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T17:35:29.517453Z","iopub.execute_input":"2022-07-20T17:35:29.517784Z","iopub.status.idle":"2022-07-20T17:35:29.947965Z","shell.execute_reply.started":"2022-07-20T17:35:29.517752Z","shell.execute_reply":"2022-07-20T17:35:29.947087Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"+---------+----+----------+------+\n|     Name| age|Experience|Salary|\n+---------+----+----------+------+\n|    Krish|  31|        10| 30000|\n|Sudhanshu|  30|         8| 25000|\n|    Sunny|  29|         4| 20000|\n|     Paul|  24|         3| 20000|\n|   Harsha|  21|         1| 15000|\n|  Shubham|  23|         2| 18000|\n|   Mahesh|null|      null| 40000|\n|     null|  34|        10| 38000|\n|     null|  36|      null|  null|\n+---------+----+----------+------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"##drop the columns\ndf_pyspark.drop('Name').show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T17:35:29.948989Z","iopub.execute_input":"2022-07-20T17:35:29.949315Z","iopub.status.idle":"2022-07-20T17:35:30.231452Z","shell.execute_reply.started":"2022-07-20T17:35:29.949282Z","shell.execute_reply":"2022-07-20T17:35:30.230463Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"+----+----------+------+\n| age|Experience|Salary|\n+----+----------+------+\n|  31|        10| 30000|\n|  30|         8| 25000|\n|  29|         4| 20000|\n|  24|         3| 20000|\n|  21|         1| 15000|\n|  23|         2| 18000|\n|null|      null| 40000|\n|  34|        10| 38000|\n|  36|      null|  null|\n+----+----------+------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"df_pyspark.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T17:35:30.232683Z","iopub.execute_input":"2022-07-20T17:35:30.232997Z","iopub.status.idle":"2022-07-20T17:35:30.446185Z","shell.execute_reply.started":"2022-07-20T17:35:30.232964Z","shell.execute_reply":"2022-07-20T17:35:30.445101Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"+---------+----+----------+------+\n|     Name| age|Experience|Salary|\n+---------+----+----------+------+\n|    Krish|  31|        10| 30000|\n|Sudhanshu|  30|         8| 25000|\n|    Sunny|  29|         4| 20000|\n|     Paul|  24|         3| 20000|\n|   Harsha|  21|         1| 15000|\n|  Shubham|  23|         2| 18000|\n|   Mahesh|null|      null| 40000|\n|     null|  34|        10| 38000|\n|     null|  36|      null|  null|\n+---------+----+----------+------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"### Drop Null values\n### By defualt how value is eqaul to any\n\ndf_pyspark1 = df_pyspark.na.drop()\ndf_pyspark1.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T17:35:30.452488Z","iopub.execute_input":"2022-07-20T17:35:30.452924Z","iopub.status.idle":"2022-07-20T17:35:30.737977Z","shell.execute_reply.started":"2022-07-20T17:35:30.452886Z","shell.execute_reply":"2022-07-20T17:35:30.736757Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"+---------+---+----------+------+\n|     Name|age|Experience|Salary|\n+---------+---+----------+------+\n|    Krish| 31|        10| 30000|\n|Sudhanshu| 30|         8| 25000|\n|    Sunny| 29|         4| 20000|\n|     Paul| 24|         3| 20000|\n|   Harsha| 21|         1| 15000|\n|  Shubham| 23|         2| 18000|\n+---------+---+----------+------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"### any==how\n### Eventhough if there is only one null value in the row it will be dropped\ndf_pyspark2 =  df_pyspark.na.drop(how=\"any\")\ndf_pyspark2.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T17:35:30.741143Z","iopub.execute_input":"2022-07-20T17:35:30.741563Z","iopub.status.idle":"2022-07-20T17:35:30.958143Z","shell.execute_reply.started":"2022-07-20T17:35:30.741525Z","shell.execute_reply":"2022-07-20T17:35:30.956937Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"+---------+---+----------+------+\n|     Name|age|Experience|Salary|\n+---------+---+----------+------+\n|    Krish| 31|        10| 30000|\n|Sudhanshu| 30|         8| 25000|\n|    Sunny| 29|         4| 20000|\n|     Paul| 24|         3| 20000|\n|   Harsha| 21|         1| 15000|\n|  Shubham| 23|         2| 18000|\n+---------+---+----------+------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"### all==how\n### IF a featrure has all the values as NULL it will be dropped\ndf_pyspark3 =  df_pyspark.na.drop(how=\"all\")\ndf_pyspark3.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T17:35:30.960233Z","iopub.execute_input":"2022-07-20T17:35:30.961987Z","iopub.status.idle":"2022-07-20T17:35:31.291755Z","shell.execute_reply.started":"2022-07-20T17:35:30.961934Z","shell.execute_reply":"2022-07-20T17:35:31.289860Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"+---------+----+----------+------+\n|     Name| age|Experience|Salary|\n+---------+----+----------+------+\n|    Krish|  31|        10| 30000|\n|Sudhanshu|  30|         8| 25000|\n|    Sunny|  29|         4| 20000|\n|     Paul|  24|         3| 20000|\n|   Harsha|  21|         1| 15000|\n|  Shubham|  23|         2| 18000|\n|   Mahesh|null|      null| 40000|\n|     null|  34|        10| 38000|\n|     null|  36|      null|  null|\n+---------+----+----------+------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"### threshold\n### If theshold value is 1 \n### Threshold = 2, at least 1 non null values should be here\n\ndf_pyspark4 = df_pyspark.na.drop(how=\"any\",thresh=1)\ndf_pyspark4.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T17:35:31.292891Z","iopub.execute_input":"2022-07-20T17:35:31.293233Z","iopub.status.idle":"2022-07-20T17:35:31.496880Z","shell.execute_reply.started":"2022-07-20T17:35:31.293199Z","shell.execute_reply":"2022-07-20T17:35:31.496037Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"+---------+----+----------+------+\n|     Name| age|Experience|Salary|\n+---------+----+----------+------+\n|    Krish|  31|        10| 30000|\n|Sudhanshu|  30|         8| 25000|\n|    Sunny|  29|         4| 20000|\n|     Paul|  24|         3| 20000|\n|   Harsha|  21|         1| 15000|\n|  Shubham|  23|         2| 18000|\n|   Mahesh|null|      null| 40000|\n|     null|  34|        10| 38000|\n|     null|  36|      null|  null|\n+---------+----+----------+------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"### threshold\n### If theshold value is 2 last column got deleted\n### Threshold = 2, at least 2 non null values should be here\n\ndf_pyspark4 = df_pyspark.na.drop(how=\"any\",thresh=2)\ndf_pyspark4.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T17:35:31.497857Z","iopub.execute_input":"2022-07-20T17:35:31.498297Z","iopub.status.idle":"2022-07-20T17:35:31.682636Z","shell.execute_reply.started":"2022-07-20T17:35:31.498264Z","shell.execute_reply":"2022-07-20T17:35:31.681691Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"+---------+----+----------+------+\n|     Name| age|Experience|Salary|\n+---------+----+----------+------+\n|    Krish|  31|        10| 30000|\n|Sudhanshu|  30|         8| 25000|\n|    Sunny|  29|         4| 20000|\n|     Paul|  24|         3| 20000|\n|   Harsha|  21|         1| 15000|\n|  Shubham|  23|         2| 18000|\n|   Mahesh|null|      null| 40000|\n|     null|  34|        10| 38000|\n+---------+----+----------+------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"### threshold\n### If theshold value is 3\n### Threshold = 3, at least 3 non null values should be here\n\ndf_pyspark4 = df_pyspark.na.drop(how=\"any\",thresh=3)\ndf_pyspark4.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T17:35:31.683638Z","iopub.execute_input":"2022-07-20T17:35:31.684080Z","iopub.status.idle":"2022-07-20T17:35:31.853802Z","shell.execute_reply.started":"2022-07-20T17:35:31.684043Z","shell.execute_reply":"2022-07-20T17:35:31.852359Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"+---------+---+----------+------+\n|     Name|age|Experience|Salary|\n+---------+---+----------+------+\n|    Krish| 31|        10| 30000|\n|Sudhanshu| 30|         8| 25000|\n|    Sunny| 29|         4| 20000|\n|     Paul| 24|         3| 20000|\n|   Harsha| 21|         1| 15000|\n|  Shubham| 23|         2| 18000|\n|     null| 34|        10| 38000|\n+---------+---+----------+------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"### threshold\n### If theshold value is 4 last column got deleted\n### Threshold = 4, at least 4 non null values should be here\n\ndf_pyspark4 = df_pyspark.na.drop(how=\"any\",thresh=4)\ndf_pyspark4.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T17:35:31.855498Z","iopub.execute_input":"2022-07-20T17:35:31.855923Z","iopub.status.idle":"2022-07-20T17:35:32.075542Z","shell.execute_reply.started":"2022-07-20T17:35:31.855888Z","shell.execute_reply":"2022-07-20T17:35:32.074618Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"+---------+---+----------+------+\n|     Name|age|Experience|Salary|\n+---------+---+----------+------+\n|    Krish| 31|        10| 30000|\n|Sudhanshu| 30|         8| 25000|\n|    Sunny| 29|         4| 20000|\n|     Paul| 24|         3| 20000|\n|   Harsha| 21|         1| 15000|\n|  Shubham| 23|         2| 18000|\n+---------+---+----------+------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"### Subset\n### Drop  NaN values only from a specific column \n\ndf_pyspark5 = df_pyspark.na.drop(how=\"any\",subset=['Experience'])\ndf_pyspark5.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T17:35:32.076650Z","iopub.execute_input":"2022-07-20T17:35:32.076979Z","iopub.status.idle":"2022-07-20T17:35:32.304232Z","shell.execute_reply.started":"2022-07-20T17:35:32.076944Z","shell.execute_reply":"2022-07-20T17:35:32.303360Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"+---------+---+----------+------+\n|     Name|age|Experience|Salary|\n+---------+---+----------+------+\n|    Krish| 31|        10| 30000|\n|Sudhanshu| 30|         8| 25000|\n|    Sunny| 29|         4| 20000|\n|     Paul| 24|         3| 20000|\n|   Harsha| 21|         1| 15000|\n|  Shubham| 23|         2| 18000|\n|     null| 34|        10| 38000|\n+---------+---+----------+------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"### Filling the Missing Value\n### If I pass a string  value in fill it will fill only string values\n\ndf_pyspark.printSchema()\ndf_pyspark6 = df_pyspark.na.fill('Missing Values')\ndf_pyspark6.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T17:35:32.305201Z","iopub.execute_input":"2022-07-20T17:35:32.305464Z","iopub.status.idle":"2022-07-20T17:35:32.560940Z","shell.execute_reply.started":"2022-07-20T17:35:32.305434Z","shell.execute_reply":"2022-07-20T17:35:32.559963Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"root\n |-- Name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- Experience: integer (nullable = true)\n |-- Salary: integer (nullable = true)\n\n+--------------+----+----------+------+\n|          Name| age|Experience|Salary|\n+--------------+----+----------+------+\n|         Krish|  31|        10| 30000|\n|     Sudhanshu|  30|         8| 25000|\n|         Sunny|  29|         4| 20000|\n|          Paul|  24|         3| 20000|\n|        Harsha|  21|         1| 15000|\n|       Shubham|  23|         2| 18000|\n|        Mahesh|null|      null| 40000|\n|Missing Values|  34|        10| 38000|\n|Missing Values|  36|      null|  null|\n+--------------+----+----------+------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"### Filling the Missing Value\n### If I pass a numeric  value in fill it will fill only numeric values\n\ndf_pyspark6 = df_pyspark.na.fill(0.0)\ndf_pyspark6.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T17:35:32.562373Z","iopub.execute_input":"2022-07-20T17:35:32.562878Z","iopub.status.idle":"2022-07-20T17:35:32.899988Z","shell.execute_reply.started":"2022-07-20T17:35:32.562837Z","shell.execute_reply":"2022-07-20T17:35:32.898858Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"+---------+---+----------+------+\n|     Name|age|Experience|Salary|\n+---------+---+----------+------+\n|    Krish| 31|        10| 30000|\n|Sudhanshu| 30|         8| 25000|\n|    Sunny| 29|         4| 20000|\n|     Paul| 24|         3| 20000|\n|   Harsha| 21|         1| 15000|\n|  Shubham| 23|         2| 18000|\n|   Mahesh|  0|         0| 40000|\n|     null| 34|        10| 38000|\n|     null| 36|         0|     0|\n+---------+---+----------+------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"### Filling the Missing Value\ndf_pyspark.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T17:35:32.901398Z","iopub.execute_input":"2022-07-20T17:35:32.901763Z","iopub.status.idle":"2022-07-20T17:35:33.083373Z","shell.execute_reply.started":"2022-07-20T17:35:32.901729Z","shell.execute_reply":"2022-07-20T17:35:33.082300Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"+---------+----+----------+------+\n|     Name| age|Experience|Salary|\n+---------+----+----------+------+\n|    Krish|  31|        10| 30000|\n|Sudhanshu|  30|         8| 25000|\n|    Sunny|  29|         4| 20000|\n|     Paul|  24|         3| 20000|\n|   Harsha|  21|         1| 15000|\n|  Shubham|  23|         2| 18000|\n|   Mahesh|null|      null| 40000|\n|     null|  34|        10| 38000|\n|     null|  36|      null|  null|\n+---------+----+----------+------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"df_pyspark.printSchema()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T17:35:33.084491Z","iopub.execute_input":"2022-07-20T17:35:33.084869Z","iopub.status.idle":"2022-07-20T17:35:33.092153Z","shell.execute_reply.started":"2022-07-20T17:35:33.084837Z","shell.execute_reply":"2022-07-20T17:35:33.091133Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"root\n |-- Name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- Experience: integer (nullable = true)\n |-- Salary: integer (nullable = true)\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**IMPUTER**","metadata":{}},{"cell_type":"code","source":"### Median\nfrom pyspark.ml.feature import Imputer\n\nimputer = Imputer(\n    inputCols=['age', 'Experience', 'Salary'], \n    outputCols=[\"{}_imputed\".format(c) for c in ['age', 'Experience', 'Salary']]\n    ).setStrategy(\"median\")","metadata":{"execution":{"iopub.status.busy":"2022-07-20T17:35:33.093665Z","iopub.execute_input":"2022-07-20T17:35:33.094075Z","iopub.status.idle":"2022-07-20T17:35:33.329866Z","shell.execute_reply.started":"2022-07-20T17:35:33.094036Z","shell.execute_reply":"2022-07-20T17:35:33.328642Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Add imputation cols to df\nimputer.fit(df_pyspark).transform(df_pyspark).show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T17:35:33.331612Z","iopub.execute_input":"2022-07-20T17:35:33.331935Z","iopub.status.idle":"2022-07-20T17:35:34.340178Z","shell.execute_reply.started":"2022-07-20T17:35:33.331904Z","shell.execute_reply":"2022-07-20T17:35:34.339317Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"+---------+----+----------+------+-----------+------------------+--------------+\n|     Name| age|Experience|Salary|age_imputed|Experience_imputed|Salary_imputed|\n+---------+----+----------+------+-----------+------------------+--------------+\n|    Krish|  31|        10| 30000|         31|                10|         30000|\n|Sudhanshu|  30|         8| 25000|         30|                 8|         25000|\n|    Sunny|  29|         4| 20000|         29|                 4|         20000|\n|     Paul|  24|         3| 20000|         24|                 3|         20000|\n|   Harsha|  21|         1| 15000|         21|                 1|         15000|\n|  Shubham|  23|         2| 18000|         23|                 2|         18000|\n|   Mahesh|null|      null| 40000|         29|                 4|         40000|\n|     null|  34|        10| 38000|         34|                10|         38000|\n|     null|  36|      null|  null|         36|                 4|         20000|\n+---------+----+----------+------+-----------+------------------+--------------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"### Mean\nfrom pyspark.ml.feature import Imputer\n\nimputer = Imputer(\n    inputCols=['age', 'Experience', 'Salary'], \n    outputCols=[\"{}_imputed\".format(c) for c in ['age', 'Experience', 'Salary']]\n    ).setStrategy(\"mean\")","metadata":{"execution":{"iopub.status.busy":"2022-07-20T17:35:34.341271Z","iopub.execute_input":"2022-07-20T17:35:34.341616Z","iopub.status.idle":"2022-07-20T17:35:34.353058Z","shell.execute_reply.started":"2022-07-20T17:35:34.341564Z","shell.execute_reply":"2022-07-20T17:35:34.352118Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Add imputation cols to df\nimputer.fit(df_pyspark).transform(df_pyspark).show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T17:35:34.354761Z","iopub.execute_input":"2022-07-20T17:35:34.355393Z","iopub.status.idle":"2022-07-20T17:35:35.471425Z","shell.execute_reply.started":"2022-07-20T17:35:34.355361Z","shell.execute_reply":"2022-07-20T17:35:35.470511Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"+---------+----+----------+------+-----------+------------------+--------------+\n|     Name| age|Experience|Salary|age_imputed|Experience_imputed|Salary_imputed|\n+---------+----+----------+------+-----------+------------------+--------------+\n|    Krish|  31|        10| 30000|         31|                10|         30000|\n|Sudhanshu|  30|         8| 25000|         30|                 8|         25000|\n|    Sunny|  29|         4| 20000|         29|                 4|         20000|\n|     Paul|  24|         3| 20000|         24|                 3|         20000|\n|   Harsha|  21|         1| 15000|         21|                 1|         15000|\n|  Shubham|  23|         2| 18000|         23|                 2|         18000|\n|   Mahesh|null|      null| 40000|         28|                 5|         40000|\n|     null|  34|        10| 38000|         34|                10|         38000|\n|     null|  36|      null|  null|         36|                 5|         25750|\n+---------+----+----------+------+-----------+------------------+--------------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}