{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-20T15:32:32.824443Z","iopub.execute_input":"2022-07-20T15:32:32.825041Z","iopub.status.idle":"2022-07-20T15:32:32.855563Z","shell.execute_reply.started":"2022-07-20T15:32:32.824897Z","shell.execute_reply":"2022-07-20T15:32:32.854649Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/pyspark-test-data/test1.csv\n/kaggle/input/pyspark-test-data/test2.csv\n/kaggle/input/pyspark-test-data/tips.csv\n/kaggle/input/pyspark-test-data/test3.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pyspark\nimport pyspark","metadata":{"execution":{"iopub.status.busy":"2022-07-20T15:32:32.863259Z","iopub.execute_input":"2022-07-20T15:32:32.863874Z","iopub.status.idle":"2022-07-20T15:32:44.936625Z","shell.execute_reply.started":"2022-07-20T15:32:32.863838Z","shell.execute_reply":"2022-07-20T15:32:44.935094Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyspark in /opt/conda/lib/python3.7/site-packages (3.3.0)\nRequirement already satisfied: py4j==0.10.9.5 in /opt/conda/lib/python3.7/site-packages (from pyspark) (0.10.9.5)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('Dataframe').getOrCreate()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T15:32:44.939211Z","iopub.execute_input":"2022-07-20T15:32:44.939939Z","iopub.status.idle":"2022-07-20T15:32:49.934971Z","shell.execute_reply.started":"2022-07-20T15:32:44.939876Z","shell.execute_reply":"2022-07-20T15:32:49.933762Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","output_type":"stream"},{"name":"stdout","text":"22/07/20 15:32:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}]},{"cell_type":"code","source":"spark","metadata":{"execution":{"iopub.status.busy":"2022-07-20T15:32:49.936282Z","iopub.execute_input":"2022-07-20T15:32:49.936637Z","iopub.status.idle":"2022-07-20T15:32:51.061648Z","shell.execute_reply.started":"2022-07-20T15:32:49.936600Z","shell.execute_reply":"2022-07-20T15:32:51.060420Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<pyspark.sql.session.SparkSession at 0x7f872dd031d0>","text/html":"\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://27f4f2d8136c:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Dataframe</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "},"metadata":{}}]},{"cell_type":"markdown","source":"**FILE READING METHODS**","metadata":{}},{"cell_type":"code","source":"## read the dataset way1 \nspark.read.option('header','true').csv('../input/pyspark-test-data/test1.csv')","metadata":{"execution":{"iopub.status.busy":"2022-07-20T15:32:51.066622Z","iopub.execute_input":"2022-07-20T15:32:51.067008Z","iopub.status.idle":"2022-07-20T15:32:56.566021Z","shell.execute_reply.started":"2022-07-20T15:32:51.066977Z","shell.execute_reply":"2022-07-20T15:32:56.564809Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DataFrame[Name: string, age: string, Experience: string, Salary: string]"},"metadata":{}}]},{"cell_type":"code","source":"## read the dataset way2\nspark.read.option('header','true').csv('../input/pyspark-test-data/test1.csv').show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T15:32:56.568213Z","iopub.execute_input":"2022-07-20T15:32:56.569077Z","iopub.status.idle":"2022-07-20T15:32:57.274472Z","shell.execute_reply.started":"2022-07-20T15:32:56.569029Z","shell.execute_reply":"2022-07-20T15:32:57.273171Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"+---------+---+----------+------+\n|     Name|age|Experience|Salary|\n+---------+---+----------+------+\n|    Krish| 31|        10| 30000|\n|Sudhanshu| 30|         8| 25000|\n|    Sunny| 29|         4| 20000|\n|     Paul| 24|         3| 20000|\n|   Harsha| 21|         1| 15000|\n|  Shubham| 23|         2| 18000|\n+---------+---+----------+------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"## read the dataset way 3\ndf_pyspark=spark.read.option('header','true').csv('../input/pyspark-test-data/test1.csv')\ndf_pyspark","metadata":{"execution":{"iopub.status.busy":"2022-07-20T15:32:57.275810Z","iopub.execute_input":"2022-07-20T15:32:57.276385Z","iopub.status.idle":"2022-07-20T15:32:57.619795Z","shell.execute_reply.started":"2022-07-20T15:32:57.276336Z","shell.execute_reply":"2022-07-20T15:32:57.618659Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DataFrame[Name: string, age: string, Experience: string, Salary: string]"},"metadata":{}}]},{"cell_type":"code","source":"### Check the schema \n### Here all are strings\ndf_pyspark.printSchema()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T15:32:57.621056Z","iopub.execute_input":"2022-07-20T15:32:57.621503Z","iopub.status.idle":"2022-07-20T15:32:57.632888Z","shell.execute_reply.started":"2022-07-20T15:32:57.621463Z","shell.execute_reply":"2022-07-20T15:32:57.631160Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"root\n |-- Name: string (nullable = true)\n |-- age: string (nullable = true)\n |-- Experience: string (nullable = true)\n |-- Salary: string (nullable = true)\n\n","output_type":"stream"}]},{"cell_type":"code","source":"## read the dataset way 4\ndf_pyspark=spark.read.option('header','true').csv('../input/pyspark-test-data/test1.csv',inferSchema=True)\ndf_pyspark","metadata":{"execution":{"iopub.status.busy":"2022-07-20T15:32:57.634803Z","iopub.execute_input":"2022-07-20T15:32:57.635708Z","iopub.status.idle":"2022-07-20T15:32:58.276209Z","shell.execute_reply.started":"2022-07-20T15:32:57.635661Z","shell.execute_reply":"2022-07-20T15:32:58.275378Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"DataFrame[Name: string, age: int, Experience: int, Salary: int]"},"metadata":{}}]},{"cell_type":"code","source":"### Check the schema\n### Here relavent columns are in Int format\ndf_pyspark.printSchema()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T15:32:58.279705Z","iopub.execute_input":"2022-07-20T15:32:58.280052Z","iopub.status.idle":"2022-07-20T15:32:58.286035Z","shell.execute_reply.started":"2022-07-20T15:32:58.280021Z","shell.execute_reply":"2022-07-20T15:32:58.285176Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"root\n |-- Name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- Experience: integer (nullable = true)\n |-- Salary: integer (nullable = true)\n\n","output_type":"stream"}]},{"cell_type":"code","source":"## read the dataset way 5\ndf_pyspark = spark.read.csv(\"../input/pyspark-test-data/test1.csv\",header = True,inferSchema=True)\ndf_pyspark.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T15:32:58.289834Z","iopub.execute_input":"2022-07-20T15:32:58.290448Z","iopub.status.idle":"2022-07-20T15:32:58.878912Z","shell.execute_reply.started":"2022-07-20T15:32:58.290414Z","shell.execute_reply":"2022-07-20T15:32:58.877680Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"+---------+---+----------+------+\n|     Name|age|Experience|Salary|\n+---------+---+----------+------+\n|    Krish| 31|        10| 30000|\n|Sudhanshu| 30|         8| 25000|\n|    Sunny| 29|         4| 20000|\n|     Paul| 24|         3| 20000|\n|   Harsha| 21|         1| 15000|\n|  Shubham| 23|         2| 18000|\n+---------+---+----------+------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"### Check the data type of the column\ndf_pyspark.printSchema()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T15:32:58.880276Z","iopub.execute_input":"2022-07-20T15:32:58.880726Z","iopub.status.idle":"2022-07-20T15:32:58.893094Z","shell.execute_reply.started":"2022-07-20T15:32:58.880680Z","shell.execute_reply":"2022-07-20T15:32:58.891683Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"root\n |-- Name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- Experience: integer (nullable = true)\n |-- Salary: integer (nullable = true)\n\n","output_type":"stream"}]},{"cell_type":"code","source":"type(df_pyspark)","metadata":{"execution":{"iopub.status.busy":"2022-07-20T15:32:58.895305Z","iopub.execute_input":"2022-07-20T15:32:58.896309Z","iopub.status.idle":"2022-07-20T15:32:58.904391Z","shell.execute_reply.started":"2022-07-20T15:32:58.896261Z","shell.execute_reply":"2022-07-20T15:32:58.903207Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"pyspark.sql.dataframe.DataFrame"},"metadata":{}}]},{"cell_type":"markdown","source":"**SELECTING COLUMNS AND INDEXING**","metadata":{}},{"cell_type":"code","source":"df_pyspark.columns","metadata":{"execution":{"iopub.status.busy":"2022-07-20T15:32:58.906416Z","iopub.execute_input":"2022-07-20T15:32:58.907430Z","iopub.status.idle":"2022-07-20T15:32:58.921204Z","shell.execute_reply.started":"2022-07-20T15:32:58.907233Z","shell.execute_reply":"2022-07-20T15:32:58.919965Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"['Name', 'age', 'Experience', 'Salary']"},"metadata":{}}]},{"cell_type":"code","source":"# Here we get in a lsit format instead of dataframe format\ndf_pyspark.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-07-20T15:32:58.923738Z","iopub.execute_input":"2022-07-20T15:32:58.924696Z","iopub.status.idle":"2022-07-20T15:32:59.089289Z","shell.execute_reply.started":"2022-07-20T15:32:58.924611Z","shell.execute_reply":"2022-07-20T15:32:59.088243Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"[Row(Name='Krish', age=31, Experience=10, Salary=30000),\n Row(Name='Sudhanshu', age=30, Experience=8, Salary=25000),\n Row(Name='Sunny', age=29, Experience=4, Salary=20000),\n Row(Name='Paul', age=24, Experience=3, Salary=20000),\n Row(Name='Harsha', age=21, Experience=1, Salary=15000)]"},"metadata":{}}]},{"cell_type":"code","source":"df_pyspark.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T15:32:59.091107Z","iopub.execute_input":"2022-07-20T15:32:59.091533Z","iopub.status.idle":"2022-07-20T15:32:59.245049Z","shell.execute_reply.started":"2022-07-20T15:32:59.091491Z","shell.execute_reply":"2022-07-20T15:32:59.243817Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"+---------+---+----------+------+\n|     Name|age|Experience|Salary|\n+---------+---+----------+------+\n|    Krish| 31|        10| 30000|\n|Sudhanshu| 30|         8| 25000|\n|    Sunny| 29|         4| 20000|\n|     Paul| 24|         3| 20000|\n|   Harsha| 21|         1| 15000|\n|  Shubham| 23|         2| 18000|\n+---------+---+----------+------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Select a column\ndf_pyspark.select('Name')","metadata":{"execution":{"iopub.status.busy":"2022-07-20T15:32:59.246371Z","iopub.execute_input":"2022-07-20T15:32:59.246849Z","iopub.status.idle":"2022-07-20T15:32:59.283857Z","shell.execute_reply.started":"2022-07-20T15:32:59.246804Z","shell.execute_reply":"2022-07-20T15:32:59.282737Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"DataFrame[Name: string]"},"metadata":{}}]},{"cell_type":"code","source":"# Select a column and show details\ndf_pyspark.select('Name').show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T15:32:59.285231Z","iopub.execute_input":"2022-07-20T15:32:59.285661Z","iopub.status.idle":"2022-07-20T15:32:59.428125Z","shell.execute_reply.started":"2022-07-20T15:32:59.285620Z","shell.execute_reply":"2022-07-20T15:32:59.426832Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"+---------+\n|     Name|\n+---------+\n|    Krish|\n|Sudhanshu|\n|    Sunny|\n|     Paul|\n|   Harsha|\n|  Shubham|\n+---------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"type(df_pyspark.select('Name'))","metadata":{"execution":{"iopub.status.busy":"2022-07-20T15:32:59.429969Z","iopub.execute_input":"2022-07-20T15:32:59.430426Z","iopub.status.idle":"2022-07-20T15:32:59.451574Z","shell.execute_reply.started":"2022-07-20T15:32:59.430377Z","shell.execute_reply":"2022-07-20T15:32:59.450445Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"pyspark.sql.dataframe.DataFrame"},"metadata":{}}]},{"cell_type":"code","source":"# Select multiple columns and show details\ndf_pyspark.select(['Name','Experience']).show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T15:32:59.453400Z","iopub.execute_input":"2022-07-20T15:32:59.454350Z","iopub.status.idle":"2022-07-20T15:32:59.776019Z","shell.execute_reply.started":"2022-07-20T15:32:59.454305Z","shell.execute_reply":"2022-07-20T15:32:59.774796Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"+---------+----------+\n|     Name|Experience|\n+---------+----------+\n|    Krish|        10|\n|Sudhanshu|         8|\n|    Sunny|         4|\n|     Paul|         3|\n|   Harsha|         1|\n|  Shubham|         2|\n+---------+----------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"### Cant do like in pandas , we always have to use select\ndf_pyspark['Name']","metadata":{"execution":{"iopub.status.busy":"2022-07-20T15:32:59.777384Z","iopub.execute_input":"2022-07-20T15:32:59.777850Z","iopub.status.idle":"2022-07-20T15:32:59.821886Z","shell.execute_reply.started":"2022-07-20T15:32:59.777804Z","shell.execute_reply":"2022-07-20T15:32:59.820657Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"Column<'Name'>"},"metadata":{}}]},{"cell_type":"code","source":"df_pyspark.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-07-20T15:32:59.823152Z","iopub.execute_input":"2022-07-20T15:32:59.823565Z","iopub.status.idle":"2022-07-20T15:32:59.841205Z","shell.execute_reply.started":"2022-07-20T15:32:59.823522Z","shell.execute_reply":"2022-07-20T15:32:59.840022Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"[('Name', 'string'), ('age', 'int'), ('Experience', 'int'), ('Salary', 'int')]"},"metadata":{}}]},{"cell_type":"code","source":"df_pyspark.describe()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T15:32:59.843493Z","iopub.execute_input":"2022-07-20T15:32:59.846246Z","iopub.status.idle":"2022-07-20T15:33:01.331280Z","shell.execute_reply.started":"2022-07-20T15:32:59.846195Z","shell.execute_reply":"2022-07-20T15:33:01.330160Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"DataFrame[summary: string, Name: string, age: string, Experience: string, Salary: string]"},"metadata":{}}]},{"cell_type":"code","source":"df_pyspark.describe().show()\n\n### Here NULL values come because it will take the string column also\n### Min and Max is taken on the index in 'Name' column \n","metadata":{"execution":{"iopub.status.busy":"2022-07-20T15:33:01.333196Z","iopub.execute_input":"2022-07-20T15:33:01.334024Z","iopub.status.idle":"2022-07-20T15:33:01.850408Z","shell.execute_reply.started":"2022-07-20T15:33:01.333977Z","shell.execute_reply":"2022-07-20T15:33:01.849187Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"+-------+------+------------------+-----------------+------------------+\n|summary|  Name|               age|       Experience|            Salary|\n+-------+------+------------------+-----------------+------------------+\n|  count|     6|                 6|                6|                 6|\n|   mean|  null|26.333333333333332|4.666666666666667|21333.333333333332|\n| stddev|  null| 4.179314138308661|3.559026084010437| 5354.126134736337|\n|    min|Harsha|                21|                1|             15000|\n|    max| Sunny|                31|               10|             30000|\n+-------+------+------------------+-----------------+------------------+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Adding Columns**","metadata":{}},{"cell_type":"code","source":"### Adding Columns in data frame\n### To get reflected we need to assign it to a new variable\n\ndf_pyspark1 = df_pyspark.withColumn('Experience After 2 years',df_pyspark['Experience']+2)\ndf_pyspark1.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T15:33:01.851511Z","iopub.execute_input":"2022-07-20T15:33:01.851961Z","iopub.status.idle":"2022-07-20T15:33:02.032516Z","shell.execute_reply.started":"2022-07-20T15:33:01.851911Z","shell.execute_reply":"2022-07-20T15:33:02.031452Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"+---------+---+----------+------+------------------------+\n|     Name|age|Experience|Salary|Experience After 2 years|\n+---------+---+----------+------+------------------------+\n|    Krish| 31|        10| 30000|                      12|\n|Sudhanshu| 30|         8| 25000|                      10|\n|    Sunny| 29|         4| 20000|                       6|\n|     Paul| 24|         3| 20000|                       5|\n|   Harsha| 21|         1| 15000|                       3|\n|  Shubham| 23|         2| 18000|                       4|\n+---------+---+----------+------+------------------------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"### Drop the columns\n### To get reflected we need to assign it to a new variable\n\ndf_pyspark2 = df_pyspark1.drop('Experience After 2 year')\ndf_pyspark.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T15:33:02.033482Z","iopub.execute_input":"2022-07-20T15:33:02.033817Z","iopub.status.idle":"2022-07-20T15:33:02.151508Z","shell.execute_reply.started":"2022-07-20T15:33:02.033787Z","shell.execute_reply":"2022-07-20T15:33:02.150355Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"+---------+---+----------+------+\n|     Name|age|Experience|Salary|\n+---------+---+----------+------+\n|    Krish| 31|        10| 30000|\n|Sudhanshu| 30|         8| 25000|\n|    Sunny| 29|         4| 20000|\n|     Paul| 24|         3| 20000|\n|   Harsha| 21|         1| 15000|\n|  Shubham| 23|         2| 18000|\n+---------+---+----------+------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"### Rename the columns\ndf_pyspark2.withColumnRenamed('Name','New Name').show()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T15:33:02.152737Z","iopub.execute_input":"2022-07-20T15:33:02.153127Z","iopub.status.idle":"2022-07-20T15:33:02.297912Z","shell.execute_reply.started":"2022-07-20T15:33:02.153087Z","shell.execute_reply":"2022-07-20T15:33:02.296652Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"+---------+---+----------+------+------------------------+\n| New Name|age|Experience|Salary|Experience After 2 years|\n+---------+---+----------+------+------------------------+\n|    Krish| 31|        10| 30000|                      12|\n|Sudhanshu| 30|         8| 25000|                      10|\n|    Sunny| 29|         4| 20000|                       6|\n|     Paul| 24|         3| 20000|                       5|\n|   Harsha| 21|         1| 15000|                       3|\n|  Shubham| 23|         2| 18000|                       4|\n+---------+---+----------+------+------------------------+\n\n","output_type":"stream"}]}]}